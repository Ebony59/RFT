{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfe1ad1e-14c2-49fc-86fe-291a5bf4b2ca",
   "metadata": {},
   "source": [
    "# Convergence ML Engineer / Researcher Take-home\n",
    "\n",
    "We'd like to learn a little more about how you practically approach a small research-like project loosely based on Rejection Sampling Fine-tuning (aka RFT, introduced in https://arxiv.org/abs/2308.01825).\n",
    "\n",
    "Tip: focus on section 3.3 (\"Rejection Sampling Fine-tuning\"). The paper isn't the best written, and we're happy to clarify anything.\n",
    "\n",
    "We will provide some skeleton code for you to guide what we would like to see from you, although if you have ideas for a different structure you feel is better or more elegant, then feel free to rewrite and replace at will.\n",
    "\n",
    "Note: your final submission does not have to be in a colab notebook, does not have to use Hugging Face, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## A note from the team\n",
    "\n",
    "We want to give you a chance to show off some of your best abilities.\n",
    "\n",
    "For some people that might mean generating high quality data in a smart way. For others, it might be speeding up the whole process to enable easy reproducibility, and maybe organizing the code in a better way than given. Yet for others, it might be a chance to show off some modern policy optimization techniques like DPO or its variants. Or maybe focusing on solid evaluations and identifying limitations of small models and limited fine-tuning.\n",
    "\n",
    "An ideal outcome of course is some sense of the model improving its mathematical abilities, but itâ€™s not a bad thing if the final evaluation somehow shows equal or worse performance ðŸ˜‚ (negative results are results).\n",
    "\n",
    "Ask lots of question! We're happy to answer any questions about the assignment, and to discuss concepts like RFT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e25593-16d2-49c1-9c58-a0a9664f19df",
   "metadata": {},
   "source": [
    "### extra package installation\n",
    "\n",
    "pip3 install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "pip install flash-attn==2.5.8 --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebe49016-c9b0-44de-997d-7774527c8ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import datasets\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94bf59c3-7605-4d4f-b966-53b408904956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "281ba694981e48edaa83e1dc0ee648d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/3.45k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c4f3398148b45a8acbcc291064ac1e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_phi3.py:   0%|          | 0.00/11.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3.5-mini-instruct:\n",
      "- configuration_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ee9713540d24f65b9954d8360d14a41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_phi3.py:   0%|          | 0.00/73.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3.5-mini-instruct:\n",
      "- modeling_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "`flash-attention` package not found, consider installing for better performance: libcudart.so.12: cannot open shared object file: No such file or directory.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c381c3b61484494786797babf942da0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/16.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "310271b9748d41d98987c82336a8409e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08f1f409cfa74c51b961cd573dc5cd83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0329ad266d04e04a5d9604576b5eb48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc1432d054fa45f0b4f99b969fdebdd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e653e23af7b49e290b94a953b086246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/195 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "791d8742c5c947318b3b37fda1d5a8ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.98k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d853caf4cc24714a2e15c83f555ea52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f99c18d92524b418146a9a01214b6ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e733dc02f6c409a9601aef00cba2a9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d7c572fde504c3eb431edf7aa207182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "861f8a5323584951b6fbb9504eee3020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/7.94k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3448ac7fe9a6432b8819e6598ee848f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/2.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d3be16859ef47a69a4d5d94af59386d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/419k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd1a3571393e4544a051c61df6d8ce0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15ffd575a7f24e6c9798659da9fa15bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.random.manual_seed(0)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3.5-mini-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\")\n",
    "\n",
    "ds = datasets.load_dataset(\"openai/gsm8k\", \"main\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e7f7347-dbc0-4e9d-8455-ef7def081fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?',\n",
       " 'answer': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2c386b5f-9e56-4762-8adf-882b378c892b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_solution(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    question,\n",
    "    k = 10,\n",
    ") -> datasets.Dataset:\n",
    "    \n",
    "    prompt = \"\"\"\n",
    "    You are an AI assistant to solve maths problems. For each question, write a step-by-step solution and give the final answer in format: solution\\n#### final_answer\n",
    "    Only include the solution and answer, do not include any other descriptions.\n",
    "    \n",
    "    Example:\n",
    "    Question:\n",
    "    Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? \n",
    "    \n",
    "    Your answer should be:\n",
    "    Natalia sold 48/2 = <<48/2=24>>24 clips in May.\n",
    "    Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\n",
    "    #### 72\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [\n",
    "      {\"role\": \"system\", \"content\": prompt},\n",
    "      {\"role\": \"user\", \"content\": \"Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\"},\n",
    "      {\"role\": \"assistant\", \"content\": 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72'},\n",
    "      {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    \n",
    "    pipe = pipeline(\n",
    "      \"text-generation\",\n",
    "      model=model,\n",
    "      tokenizer=tokenizer\n",
    "    )\n",
    "    \n",
    "    generation_args = {\n",
    "      \"max_new_tokens\": 500,\n",
    "      \"return_full_text\": False,\n",
    "      \"do_sample\": True,\n",
    "      \"temperature\": 0.7,\n",
    "    }\n",
    "\n",
    "    responses = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        print(f\"############{i}############\")\n",
    "        output = pipe(messages, **generation_args)\n",
    "        text = output[0]['generated_text']\n",
    "        print(text)\n",
    "\n",
    "        responses.append(text)\n",
    "\n",
    "    return responses\n",
    "        \n",
    "\n",
    "def generate_synthetic_data(model, tokenizer, questions, answers, k=10):\n",
    "    dataset = []\n",
    "    for question, answer in zip(questions, answers):\n",
    "        print('question:', question)\n",
    "        print('answer:', answer)\n",
    "        gt_answer = answer.split(\"####\")[1].lstrip(' ').strip()\n",
    "        print('gt_answer:', gt_answer)\n",
    "        responses = generate_solution(model, tokenizer, question, k)\n",
    "        for i, response in enumerate(responses):\n",
    "            response_answer = response.split(\"####\")[1].lstrip(' ').strip()\n",
    "            if gt_answer == response_answer:\n",
    "                correct = True\n",
    "            else:\n",
    "                correct = False\n",
    "            dataset.append({'question': question, 'gt_answer': answer, 'llm_answer': response, 'correct': correct})\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "be14f61d-d97e-40d5-baf1-57d71ee3955a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\n",
      "answer: Weng earns 12/60 = $<<12/60=0.2>>0.2 per minute.\n",
      "Working 50 minutes, she earned 0.2 x 50 = $<<0.2*50=10>>10.\n",
      "#### 10\n",
      "gt_answer: 10\n",
      "############0############\n",
      " To find out how much Weng earned, we first need to convert the minutes she worked into hours. There are 60 minutes in an hour, so:\n",
      "\n",
      "50 minutes Ã· 60 minutes/hour = 0.8333 hours (rounded to four decimal places)\n",
      "\n",
      "Now, we can calculate her earnings by multiplying the hours worked by her hourly rate:\n",
      "\n",
      "0.8333 hours Ã— $12/hour = $10.00 (rounded to two decimal places)\n",
      "\n",
      "Weng earned $10.00 for 50 minutes of babysitting yesterday.\n",
      "#### 10.00\n"
     ]
    }
   ],
   "source": [
    "dataset = generate_synthetic_data(model, tokenizer, ds['train']['question'][1:2], ds['train']['answer'][1:2], k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e5942cad-0671-4311-8cf5-77153aef6c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_chat_template(\n",
    "    example,\n",
    "    tokenizer,\n",
    "):\n",
    "    messages = [\n",
    "      {\"role\": \"user\", \"content\": example['question']},\n",
    "      {\"role\": \"assistant\", \"content\": example['answer']},\n",
    "    ]\n",
    "    \n",
    "    example[\"text\"] = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=False)\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "86392dc7-9e63-4669-9d88-82433274d4b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0870dbc5290a4ed39a92b0f2d2b24a01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train_sft (num_proc=10):   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a0cb04d832f45c285a2ef19b4769c70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train_sft (num_proc=10):   0%|          | 0/1319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_1 = ds.map(\n",
    "    apply_chat_template,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer},\n",
    "    num_proc=10,\n",
    "    # remove_columns=column_names,\n",
    "    desc=\"Applying chat template to train_sft\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6e23a664-8da6-4e83-9d7a-71371883f248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?',\n",
       " 'answer': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72',\n",
       " 'text': '<|user|>\\nNatalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?<|end|>\\n<|assistant|>\\nNatalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72<|end|>\\n<|endoftext|>'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_1['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447ddea5-05ab-4a89-86c9-0467113aae75",
   "metadata": {},
   "source": [
    "### Training on the Collected Data\n",
    "\n",
    "Of course, the computational resources of colab are limited.\n",
    "\n",
    "Employ whatever trick you would like to reduce the VRAM requirements during training (including swapping the model for a smaller one, although please only as a last resort)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df144b0-1394-4fa4-a504-ec4e27d8d015",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    lr: float = 3e-5\n",
    "    epochs: int = 2\n",
    "    batch_size: int = 4\n",
    "    device: str = 'cpu'\n",
    "\n",
    "def train(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    dataset: datasets.Dataset,\n",
    "    config: TrainConfig,\n",
    ") -> AutoModelForCausalLM:\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        # Implement this\n",
    "        return\n",
    "\n",
    "    def loss_fn(batch):\n",
    "        # Implement an appropriate loss - note we don't expect this to necessarily\n",
    "        # be tied to the earlier mentioned paper, just something that is sensible\n",
    "        return\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=config.lr)\n",
    "\n",
    "    dataloader = DataLoader(dataset, collate_fn=collate_fn, batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "    # This is a pretty bare-bones loop, feel free to add anything particularly useful\n",
    "    for epoch in config.epochs:\n",
    "        model.train()\n",
    "\n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss = loss_fn(**batch)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "    return model\n",
    "\n",
    "amazing_model = train(model, tokenizer, synthetic_dataset, TrainConfig())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d3391b-fc3d-4c88-9f81-e586c8d770b8",
   "metadata": {},
   "source": [
    "### Evaluating the Model\n",
    "\n",
    "This final part is more free-form. We'd like to evaluate our new model on the test set to see if it's improved, but then spend however much time you have left examining the model more closely / demonstrating some interesting behaviour / showing off beautiful plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df63b36-a784-4bf4-abe0-055ec4c15353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    model: AutoModelForCausalLM,\n",
    "    eval_dataset: datasets.Dataset,\n",
    ") -> float:\n",
    "    return 0.0\n",
    "\n",
    "our_score = evaluate_model(amazing_model, ds['test'])\n",
    "original_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3.5-mini-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "their_score = evaluate_model(original_model, ds['test'])\n",
    "\n",
    "conclusion = 'ðŸŽ‰ðŸŽ‰ðŸŽ‰' if our_score > their_score else 'oh well, was it even supposed to work?'\n",
    "print(conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcae8e7-3a95-4e25-affb-40348557ea9b",
   "metadata": {},
   "source": [
    "### [Optional] - Discussion\n",
    "\n",
    "We would be interested to know:\n",
    "\n",
    "1.   If you were less time / computationally constrained, what would you do differently?\n",
    "2.   What would your ideal first project look like if you joined?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7400ba09-ca2d-4e32-a6fb-e2e9460844b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
